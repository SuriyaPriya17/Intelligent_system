{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzyRQ1lvGSgu/t0t21LA/W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuriyaPriya17/Intelligent_system/blob/main/text_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqQrt0hdEb2O",
        "outputId": "cf28a06f-7017-4800-d531-323f8482df04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== ORIGINAL TEXT ==========\n",
            "\n",
            "\n",
            "Climate change is one of the biggest challenges facing our planet today.\n",
            "Rising temperatures, melting glaciers, and extreme weather events threaten ecosystems and human livelihoods.\n",
            "Governments and organizations across the world are working together to reduce carbon emissions and promote renewable energy sources.\n",
            "However, real change requires collective action — individuals must also adopt sustainable habits like recycling, using public transport, and conserving water.\n",
            "The time to act is now! #ClimateAction #GoGreen\n",
            "\n",
            "\n",
            "========== TOKENIZATION ==========\n",
            "\n",
            "Sentence Tokenization:\n",
            "['\\nClimate change is one of the biggest challenges facing our planet today.', 'Rising temperatures, melting glaciers, and extreme weather events threaten ecosystems and human livelihoods.', 'Governments and organizations across the world are working together to reduce carbon emissions and promote renewable energy sources.', 'However, real change requires collective action — individuals must also adopt sustainable habits like recycling, using public transport, and conserving water.', 'The time to act is now!', '#ClimateAction #GoGreen'] \n",
            "\n",
            "Word Tokenization:\n",
            "['Climate', 'change', 'is', 'one', 'of', 'the', 'biggest', 'challenges', 'facing', 'our', 'planet', 'today', '.', 'Rising', 'temperatures', ',', 'melting', 'glaciers', ',', 'and', 'extreme', 'weather', 'events', 'threaten', 'ecosystems', 'and', 'human', 'livelihoods', '.', 'Governments', 'and', 'organizations', 'across', 'the', 'world', 'are', 'working', 'together', 'to', 'reduce', 'carbon', 'emissions', 'and', 'promote', 'renewable', 'energy', 'sources', '.', 'However', ',', 'real', 'change', 'requires', 'collective', 'action', '—', 'individuals', 'must', 'also', 'adopt', 'sustainable', 'habits', 'like', 'recycling', ',', 'using', 'public', 'transport', ',', 'and', 'conserving', 'water', '.', 'The', 'time', 'to', 'act', 'is', 'now', '!', '#', 'ClimateAction', '#', 'GoGreen'] \n",
            "\n",
            "Regex Tokenization (Words Only):\n",
            "['Climate', 'change', 'is', 'one', 'of', 'the', 'biggest', 'challenges', 'facing', 'our', 'planet', 'today', 'Rising', 'temperatures', 'melting', 'glaciers', 'and', 'extreme', 'weather', 'events', 'threaten', 'ecosystems', 'and', 'human', 'livelihoods', 'Governments', 'and', 'organizations', 'across', 'the', 'world', 'are', 'working', 'together', 'to', 'reduce', 'carbon', 'emissions', 'and', 'promote', 'renewable', 'energy', 'sources', 'However', 'real', 'change', 'requires', 'collective', 'action', 'individuals', 'must', 'also', 'adopt', 'sustainable', 'habits', 'like', 'recycling', 'using', 'public', 'transport', 'and', 'conserving', 'water', 'The', 'time', 'to', 'act', 'is', 'now', 'ClimateAction', 'GoGreen'] \n",
            "\n",
            "Tweet Tokenization:\n",
            "['Climate', 'change', 'is', 'one', 'of', 'the', 'biggest', 'challenges', 'facing', 'our', 'planet', 'today', '.', 'Rising', 'temperatures', ',', 'melting', 'glaciers', ',', 'and', 'extreme', 'weather', 'events', 'threaten', 'ecosystems', 'and', 'human', 'livelihoods', '.', 'Governments', 'and', 'organizations', 'across', 'the', 'world', 'are', 'working', 'together', 'to', 'reduce', 'carbon', 'emissions', 'and', 'promote', 'renewable', 'energy', 'sources', '.', 'However', ',', 'real', 'change', 'requires', 'collective', 'action', '—', 'individuals', 'must', 'also', 'adopt', 'sustainable', 'habits', 'like', 'recycling', ',', 'using', 'public', 'transport', ',', 'and', 'conserving', 'water', '.', 'The', 'time', 'to', 'act', 'is', 'now', '!', '#ClimateAction', '#GoGreen'] \n",
            "\n",
            "\n",
            "========== STOP-WORD REMOVAL ==========\n",
            "\n",
            "After Default Stop-word Removal:\n",
            "['Climate', 'change', 'one', 'biggest', 'challenges', 'facing', 'planet', 'today', 'Rising', 'temperatures', 'melting', 'glaciers', 'extreme', 'weather', 'events', 'threaten', 'ecosystems', 'human', 'livelihoods', 'Governments', 'organizations', 'across', 'world', 'working', 'together', 'reduce', 'carbon', 'emissions', 'promote', 'renewable', 'energy', 'sources', 'However', 'real', 'change', 'requires', 'collective', 'action', 'individuals', 'must', 'also', 'adopt', 'sustainable', 'habits', 'like', 'recycling', 'using', 'public', 'transport', 'conserving', 'water', 'time', 'act', 'ClimateAction', 'GoGreen'] \n",
            "\n",
            "\n",
            "========== STEMMING ==========\n",
            "\n",
            "Porter Stemmer Output:\n",
            "['climat', 'chang', 'one', 'biggest', 'challeng', 'face', 'planet', 'today', 'rise', 'temperatur', 'melt', 'glacier', 'extrem', 'weather', 'event', 'threaten', 'ecosystem', 'human', 'livelihood', 'govern', 'organ', 'across', 'world', 'work', 'togeth', 'reduc', 'carbon', 'emiss', 'promot', 'renew', 'energi', 'sourc', 'howev', 'real', 'chang', 'requir', 'collect', 'action', 'individu', 'must', 'also', 'adopt', 'sustain', 'habit', 'like', 'recycl', 'use', 'public', 'transport', 'conserv', 'water', 'time', 'act', 'climateact', 'gogreen'] \n",
            "\n",
            "\n",
            "========== LEMMATIZATION ==========\n",
            "\n",
            "Lemmatization Output:\n",
            "['climate', 'change', 'one', 'biggest', 'challenge', 'facing', 'planet', 'today', 'rising', 'temperature', 'melting', 'glacier', 'extreme', 'weather', 'event', 'threaten', 'ecosystem', 'human', 'livelihood', 'government', 'organization', 'across', 'world', 'working', 'together', 'reduce', 'carbon', 'emission', 'promote', 'renewable', 'energy', 'source', 'however', 'real', 'change', 'requires', 'collective', 'action', 'individual', 'must', 'also', 'adopt', 'sustainable', 'habit', 'like', 'recycling', 'using', 'public', 'transport', 'conserving', 'water', 'time', 'act', 'climateaction', 'gogreen'] \n",
            "\n",
            "\n",
            "========== FINAL PREPROCESSED TEXT ==========\n",
            "\n",
            "climat chang one biggest challeng face planet today rise temperatur melt glacier extrem weather event threaten ecosystem human livelihood govern organ across world work togeth reduc carbon emiss promot renew energi sourc howev real chang requir collect action individu must also adopt sustain habit like recycl use public transport conserv water time act climateact gogreen\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import (\n",
        "    word_tokenize,\n",
        "    sent_tokenize,\n",
        "    RegexpTokenizer,\n",
        "    TweetTokenizer\n",
        ")\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "text = \"\"\"\n",
        "Climate change is one of the biggest challenges facing our planet today.\n",
        "Rising temperatures, melting glaciers, and extreme weather events threaten ecosystems and human livelihoods.\n",
        "Governments and organizations across the world are working together to reduce carbon emissions and promote renewable energy sources.\n",
        "However, real change requires collective action — individuals must also adopt sustainable habits like recycling, using public transport, and conserving water.\n",
        "The time to act is now! #ClimateAction #GoGreen\n",
        "\"\"\"\n",
        "\n",
        "print(\"========== ORIGINAL TEXT ==========\\n\")\n",
        "print(text)\n",
        "\n",
        "print(\"\\n========== TOKENIZATION ==========\\n\")\n",
        "\n",
        "sent_tokens = sent_tokenize(text)\n",
        "print(\"Sentence Tokenization:\")\n",
        "print(sent_tokens, \"\\n\")\n",
        "\n",
        "word_tokens = word_tokenize(text)\n",
        "print(\"Word Tokenization:\")\n",
        "print(word_tokens, \"\\n\")\n",
        "\n",
        "regexp_tokenizer = RegexpTokenizer(r'\\w+')\n",
        "regexp_tokens = regexp_tokenizer.tokenize(text)\n",
        "print(\"Regex Tokenization (Words Only):\")\n",
        "print(regexp_tokens, \"\\n\")\n",
        "\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "tweet_tokens = tweet_tokenizer.tokenize(text)\n",
        "print(\"Tweet Tokenization:\")\n",
        "print(tweet_tokens, \"\\n\")\n",
        "\n",
        "print(\"\\n========== STOP-WORD REMOVAL ==========\\n\")\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "filtered_tokens = [word for word in regexp_tokens if word.lower() not in stop_words]\n",
        "print(\"After Default Stop-word Removal:\")\n",
        "print(filtered_tokens, \"\\n\")\n",
        "print(\"\\n========== STEMMING ==========\\n\")\n",
        "\n",
        "ps = PorterStemmer()\n",
        "porter_stems = [ps.stem(word) for word in filtered_tokens]\n",
        "print(\"Porter Stemmer Output:\")\n",
        "print(porter_stems, \"\\n\")\n",
        "\n",
        "print(\"\\n========== LEMMATIZATION ==========\\n\")\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(word.lower()) for word in filtered_tokens]\n",
        "print(\"Lemmatization Output:\")\n",
        "print(lemmas, \"\\n\")\n",
        "print(\"\\n========== FINAL PREPROCESSED TEXT ==========\\n\")\n",
        "\n",
        "final_clean_text = \" \".join(porter_stems)\n",
        "print(final_clean_text)\n"
      ]
    }
  ]
}